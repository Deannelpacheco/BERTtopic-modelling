{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FUkpKGWkXmQ"
      },
      "source": [
        "# BERTopic \n",
        "To dig deeper into BERTopic, check out these sources:\n",
        "\n",
        "* Introductory blog post: https://www.maartengrootendorst.com/blog/bertopictutorial/\n",
        "* For documentation see here: https://maartengr.github.io/BERTopic/index.html\n",
        "* For a comparison with LDA, NMF, and Top2Vec see here: https://www.frontiersin.org/articles/10.3389/fsoc.2022.886498/full\n",
        "\n",
        "To run BERTopic in SANE, we are using two A10 GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd5_YE36kXmR"
      },
      "source": [
        "### Set-up\n",
        "Let's first import BERTopic and the other libraries we will need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc0OX_KokXmR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "from bertopic import BERTopic\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV45dDPXkXmR"
      },
      "source": [
        "Let's load our data from the local storage directory. Be aware that the size of the data can impact performance! Try to first experiment with smaller sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAKWI_jfkXmR"
      },
      "outputs": [],
      "source": [
        "# Load the CSV dataset into a DataFrame\n",
        "df = pd.read_csv('Path_to_your_file')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYua5NS9kXmR"
      },
      "source": [
        "### Data preparation\n",
        "\n",
        "We first start to do some basic preparations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjRGM7fGkXmR"
      },
      "source": [
        "#### Removing retweets\n",
        "\n",
        "Optional: We can start with removing retweets. This allows to focus more on the content and a fine-grained topic analysis. To investigate the \"visibility\" of tweets, we can return later on to a keyword-based analysis once we discovered the topics. For example, we can use representative sentences, that are output of the topic modeling, to later search their frequency across the original dataset.\n",
        "\n",
        "Important: Removing retweets will also improve performance since it reduces the size of our data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLeN5kWPkXmR",
        "outputId": "6af956d4-0176-4e38-c2d5-10a759e4a6d3"
      },
      "outputs": [],
      "source": [
        "print(\"Amount of tweets before removing RTs: \" + str(len(df)))\n",
        "\n",
        "df = df[~df[\"text\"].str.contains(\"RT \")]\n",
        "\n",
        "print(\"Amount of tweets after removing RTs: \" + str(len(df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN_DmujYkXmS"
      },
      "source": [
        "#### Sampling tweets\n",
        "\n",
        "Optional 1: We might want to use with a smaller sample for testing methods more efficiently. We can select the top n tweets or get a random sample of n tweets. Generally, 10k data works best for BERTopic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-iCPJ6SkXmS"
      },
      "outputs": [],
      "source": [
        "sample_size = 10000 # Set the sample size to the desired number of tweets\n",
        "\n",
        "# Random sample\n",
        "df_sample = df.sample(n=sample_size) # automatically excludes duplicates unless replace=True\n",
        "\n",
        "# Sample top n (if data is sorted by timestamp, this will create a chronological subset)\n",
        "# df_sample = df_sample[:sample_size]\n",
        "\n",
        "df_sample.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKHoDp8-kXmS"
      },
      "source": [
        "Optional 2: Or we can also assign the original df to df_sample to keep all the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d6cl9SOckXmS"
      },
      "outputs": [],
      "source": [
        "df_sample = df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhHoTHyJkXmS"
      },
      "source": [
        "#### Get full tweets\n",
        "For our tweet text, we sometimes have full tweet text (if not, the value is NaN). In general, we want to use the full tweet text whenever it is possible. We therefore fill the empty gaps for the full tweet text with the normal text and will then continue to use the full tweet text column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9qNRfmwkXmS",
        "outputId": "edb66d4f-18e1-4f77-ebae-d3a3944f74c8"
      },
      "outputs": [],
      "source": [
        "df_sample['full_text'].fillna(df_sample['text'], inplace=True)\n",
        "\n",
        "df_sample[['text', 'full_text']].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR6ZQevrkXmS"
      },
      "source": [
        "### Data cleaning\n",
        "Now we can turn to the actual cleaning of text. You can adapt the cleaning based on your needs. In this example, we adapt the cleaning to the structure of Twitter data. When using another source (e.g., television transcripts), you need to adapt this accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERzSZNRwkXmS",
        "outputId": "2dcf5f32-412d-45b8-8c43-50d4a6e07679"
      },
      "outputs": [],
      "source": [
        "df_sample[\"text_clean\"] = df_sample.apply(lambda row: row[\"full_text\"].lower(), axis=1)  # all lowercase\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove RT\n",
        "    lambda row: re.sub(r\"\\brt\\b\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove mentions\n",
        "    lambda row: re.sub(\"@[A-Za-z0-9_]+\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove links\n",
        "    lambda row: re.sub(r\"http\\S+\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = (\n",
        "    df_sample.apply(  # remove non-alphanumerical characters (except hashtags)\n",
        "        lambda row: re.sub(\"[^a-z0-9]\\\\#'\", \"\", row[\"text_clean\"]), axis=1\n",
        "    )\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove brackets\n",
        "    lambda row: re.sub(r\"[\\([{})\\]]\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove other arbitrary characters\n",
        "    lambda row: re.sub(r\"[_.!,'Ã¢â‚¬â„¢...Ã¢â‚¬Â¦]\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply( # remove more arbitrary characters (caution: you might want to keep question marks)\n",
        "    lambda row: re.sub(r\":()?\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply( # remove spaces\n",
        "    lambda row: re.sub(r\"\\xa0\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove tabs\n",
        "    lambda row: re.sub(r\"\\n\", \" \", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove hyphens\n",
        "    lambda row: re.sub(r\"-\", \" \", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove asteriks\n",
        "    lambda row: re.sub(r\"\\*\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove percentage\n",
        "    lambda row: re.sub(r\"%\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove ampersand\n",
        "    lambda row: re.sub(r\"&amp;\", \" \", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove quotation marks\n",
        "    lambda row: re.sub(r'\"', \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove quotation marks\n",
        "    lambda row: re.sub(r\"'\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove quotation marks\n",
        "    lambda row: re.sub(r\"â€œ\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(  # remove quotation marks\n",
        "    lambda row: re.sub(r\"â€™\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "emoji_pattern = re.compile(  # remove emojis [not sure if this is working]\n",
        "    \"[\"\n",
        "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "    \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "    \"\\U00002702-\\U000027B0\"\n",
        "    \"\\U000024C2-\\U0001F251\"\n",
        "    \"]+\",\n",
        "    flags=re.UNICODE,\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply(\n",
        "    lambda row: re.sub(emoji_pattern, \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply( # remove emojis like \\u200d\n",
        "    lambda row: re.sub(r\"[^\\w\\s,]\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "df_sample[\"text_clean\"] = df_sample.apply( # remove all digits (caution: removes years e.g. 2022; you might want to skip this)\n",
        "    lambda row: re.sub(r\"[0-9]\", \"\", row[\"text_clean\"]), axis=1\n",
        ")\n",
        "\n",
        "# Fix artifacts from cleaning procedure\n",
        "df_sample[\"text_clean\"] = df_sample.apply( # remove possibly empty spaces in string that result from cleaning\n",
        "    lambda row: re.sub(r\"\\s+\", \" \", row[\"text_clean\"].strip()), axis=1\n",
        ")\n",
        "\n",
        "print(\"Data cleaning finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj9k1uHSkXmS"
      },
      "source": [
        "Let's display the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVvl-qNAkXmS",
        "outputId": "c8e49777-4287-400a-94f6-c0d67bd903b2"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None) # to display full content\n",
        "df_sample[[\"text\", \"full_text\", \"text_clean\"]].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxhSV0ijkXmS"
      },
      "source": [
        "Optional: We can again check for, and remove, duplicate documents that might have fallen through the previous filtering (RT for tweets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uPUNHjFkXmT",
        "outputId": "df217110-62a8-43fa-d928-798016215de2"
      },
      "outputs": [],
      "source": [
        "print('Amount of documents before removing duplicates: ' + str(len(df_sample)))\n",
        "\n",
        "df_sample = df_sample.drop_duplicates(subset='text_clean', keep='first')\n",
        "\n",
        "print('Amount of documents after removing duplicates: ' + str(len(df_sample)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vquw18PPkXmT"
      },
      "source": [
        "Optional: If your docs are longer texts, we can split them into paragraphs at the newline symbol ('\\n') (or any other symbol of your choice). Be careful: this can create empty strings!\n",
        "\n",
        "Since we cleaned our documents for the newline symbol, we will not apply this technique here. However, feel free to change this in the  cleaning procedure above and run this cell afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cFwYTB-kXmT"
      },
      "outputs": [],
      "source": [
        "split_paragraphs = []\n",
        "\n",
        "# Create new rows for each split text\n",
        "for _, row in df_sample.iterrows():\n",
        "    paragraphs = row['text_clean'].split('\\n')\n",
        "    for para in paragraphs:\n",
        "        new_row = row.copy() # duplicate existing rows with all metadata\n",
        "        new_row['text_clean'] = para # overwrite the \"text_clean\" column with the split text\n",
        "        split_paragraphs.append(new_row)\n",
        "\n",
        "df_sample = pd.DataFrame(split_paragraphs)\n",
        "\n",
        "df_sample[:4]\n",
        "len(df_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cBaPUpMkXmT"
      },
      "source": [
        "If you have split the texts, let's remove rows with possibly empty strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A-_1RCFkXmT"
      },
      "outputs": [],
      "source": [
        "print('Amount of documents before removing empty strings: ' + str(len(df_sample)))\n",
        "\n",
        "df_sample = df_sample[df_sample['text_clean'].str.strip() != '']\n",
        "\n",
        "print('Amount of documents after removing empty strings: ' + str(len(df_sample)))\n",
        "\n",
        "df_sample[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nice! Data are well-prepared. Now let's first check the distribution of data we need for the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure the 'timestamp' column in df_sample is in datetime format\n",
        "df_sample['timestamp'] = pd.to_datetime(df_sample['timestamp'], errors='coerce')\n",
        "\n",
        "# Drop rows with invalid or missing timestamps in df_sample\n",
        "df_sample = df_sample.dropna(subset=['timestamp'])\n",
        "\n",
        "# Filter the data in df_sample to include only rows within the desired timeframe\n",
        "start_date = '2018-01-01'\n",
        "end_date = '2022-12-31'\n",
        "df_filtered = df_sample[(df_sample['timestamp'] >= start_date) & (df_sample['timestamp'] <= end_date)]\n",
        "\n",
        "# Check the timeframe of the filtered data\n",
        "min_date = df_filtered['timestamp'].min()\n",
        "max_date = df_filtered['timestamp'].max()\n",
        "\n",
        "print(f\"Filtered data timeframe: {min_date} to {max_date}\")\n",
        "print(f\"Number of rows in the filtered data: {len(df_filtered)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Getting subset using the Reg-ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a regex pattern to match words containing \"algo\", \"auto\", \"ai\", \"fout\", or \"tech\"\n",
        "regex_pattern = r\"\\b()\"\n",
        "# Apply the regex pattern to filter rows\n",
        "df_filtered_regex = df_filtered[df_filtered['text_clean'].str.contains(regex_pattern, case=False, na=False)]\n",
        "\n",
        "# Count the number of tweets that match the regex pattern\n",
        "matching_tweets_count = len(df_filtered_regex)\n",
        "\n",
        "# Print the count\n",
        "print(f\"Number of tweets matching the regex pattern: {matching_tweets_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.interpolate import make_interp_spline\n",
        "from datetime import datetime\n",
        "\n",
        "# Step 0: Define the regex pattern\n",
        "regex_pattern = r\"()\"\n",
        "\n",
        "# Step 1: Filter rows based on the regex pattern\n",
        "# Assuming the column to search is named 'text'\n",
        "df_filtered_regex = df_sample[df_sample['text'].str.contains(regex_pattern, case=False, na=False)]\n",
        "\n",
        "# Step 2: Convert timestamp column\n",
        "# Ensure the 'timestamp' column is in datetime format\n",
        "df_sample['timestamp'] = pd.to_datetime(df_sample['timestamp'], errors='coerce')\n",
        "\n",
        "# Drop rows with invalid or missing timestamps\n",
        "df_sample = df_sample.dropna(subset=['timestamp'])\n",
        "\n",
        "# Filter the data to include only rows within the desired timeframe\n",
        "start_date = '2018-01-01'\n",
        "end_date = '2022-12-31'\n",
        "df_filtered = df_sample[(df_sample['timestamp'] >= start_date) & (df_sample['timestamp'] <= end_date)]\n",
        "\n",
        "# Filter df_filtered_regex to match the timeframe of df_filtered\n",
        "df_filtered_regex = df_filtered_regex[df_filtered_regex['timestamp'].between(start_date, end_date)]\n",
        "\n",
        "# Check the timeframe of the filtered data\n",
        "min_date = df_filtered['timestamp'].min()\n",
        "max_date = df_filtered['timestamp'].max()\n",
        "\n",
        "print(f\"Filtered data timeframe: {min_date} to {max_date}\")\n",
        "print(f\"Number of rows in the filtered data: {len(df_filtered)}\")\n",
        "\n",
        "# Step 4: Resample every 4 months\n",
        "df_filtered_regex = df_filtered_regex.set_index('timestamp')\n",
        "four_month_counts = df_filtered_regex.resample('4MS').size().reset_index()\n",
        "four_month_counts.columns = ['Date', 'Count']\n",
        "\n",
        "# Step 5: Manually add a zero or flat continuation at 2023-01-01\n",
        "end_pad_date = pd.Timestamp('2023-01-01')\n",
        "four_month_counts = pd.concat([\n",
        "    four_month_counts,\n",
        "    pd.DataFrame({'Date': [end_pad_date], 'Count': [0]})\n",
        "], ignore_index=True)\n",
        "\n",
        "# Step 6: Prepare for spline interpolation\n",
        "x = four_month_counts['Date'].map(datetime.toordinal).values\n",
        "y = four_month_counts['Count'].values\n",
        "x_smooth = np.linspace(x.min(), x.max(), 500)\n",
        "\n",
        "spline = make_interp_spline(x, y, k=3)\n",
        "y_smooth = spline(x_smooth)\n",
        "\n",
        "# Clip negative values\n",
        "y_smooth = np.clip(y_smooth, 0, None)\n",
        "x_smooth_dates = [datetime.fromordinal(int(val)) for val in x_smooth]\n",
        "\n",
        "# Step 7: Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.fill_between(x_smooth_dates, y_smooth, alpha=0.6)\n",
        "plt.title('Distribution of Regex-Matched Tweets Over Time', fontsize=16)\n",
        "plt.grid(True)\n",
        "plt.xlim([datetime(2018, 1, 1), datetime(2023, 1, 1)])  # âœ… lock x-axis to stop at Jan 1, 2023\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_filtered_regex = df_filtered_regex.drop_duplicates()\n",
        "\n",
        "# Count the total number of tweets before applying the regex\n",
        "total_tweets_before = len(df_sample)\n",
        "\n",
        "# Filter the DataFrame using the regex\n",
        "df_filtered = df_sample[df_sample['text_clean'].str.contains(regex_pattern, case=False, na=False)]\n",
        "\n",
        "# Count the total number of tweets after applying the regex\n",
        "total_tweets_after = len(df_filtered_regex)\n",
        "\n",
        "# Count the total number of tweets after dropping duplicates\n",
        "total_tweets_after_dropping_duplicates = len(df_filtered_regex)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Total tweets before applying regex: {total_tweets_before}\")\n",
        "print(f\"Total tweets after applying regex: {total_tweets_after}\")\n",
        "print(f\"Number of tweets extracted using regex: {total_tweets_after}\")\n",
        "print(f\"Number of tweets after dropping duplicates: {total_tweets_after_dropping_duplicates}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_filtered_regex['text_clean'].head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBmBeb2PIcA7"
      },
      "source": [
        "To have better results from BERTopic, in the following section, we have prepared a BERTopic tester for you to test all the parameters. Then, there is a BERTopic batch runner to first split large dataset to several batches and run them one by one automatically.\n",
        "\n",
        "Please go head!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJEbL8WJkXmT"
      },
      "source": [
        "### Automatic BERTopic Batch Runner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EozuZt6aMAi7"
      },
      "source": [
        "#### Step 1: Load a Sentence Transformer for Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHvQUBinWr6a"
      },
      "source": [
        "We first load a multilingual model from sentence-transformers. This model helps convert text into embeddings â€” high-dimensional vectors that capture semantic meaning.\n",
        "\n",
        "**Different Sentence transformer models for embeddings**\n",
        "\n",
        "You can use different SentenceTransformer (aka SBERT) embedding models.\n",
        "\n",
        "Multilingual Models: Handle multiple languages for cross-lingual tasks.\n",
        "- \"paraphrase-multilingual-mpnet-base-v2\", \"distiluse-base-multilingual-cased-v1\"\n",
        "\n",
        "Paraphrase Models: Detect sentence similarity or paraphrases.\n",
        " - \"paraphrase-MiniLM-L6-v2\", \"paraphrase-distilroberta-base-v1\"\n",
        "\n",
        "Task-Specific Models: Fine-tuned for specific NLP tasks like sentiment analysis.\n",
        " - \"all-MiniLM-L6-v2\", \"all-distilroberta-v1\"\n",
        "\n",
        "Domain-Specific Models: Tailored for specific fields like law or medicine.\n",
        " - \"legal-bert-base-uncased\", \"biobert-base-cased-v1.1\"\n",
        "\n",
        "You can also go [here](https://huggingface.co/models?library=sentence-transformers) for a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-jBLoDmBkXmT"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sentence_model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")  # or your preferred embedding model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVXP-qKRk9cS"
      },
      "source": [
        "BERTopic is a powerful Python library for topic modeling â€” it can find and group similar topics in large amounts of text. One of its strengths is that it works surprisingly well with messy, online content (like tweets), even when that content includes multiple languages, spam, and a wide range of writing styles.\n",
        "\n",
        "While you can get good results using the default settings, BERTopic is very modular, which means you can customize many parts of the process to fine-tune your results â€” if you have the experience. This tuned variant is based on [this tutorial](https://towardsdatascience.com/using-whisper-and-bertopic-to-model-kurzgesagts-videos-7d8a63139bdf) and the [BERTopic documentation](https://maartengr.github.io/BERTopic/), particularly the [best practices](https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html), [algorithm](https://maartengr.github.io/BERTopic/algorithm/algorithm.html), and [FAQ](https://maartengr.github.io/BERTopic/faq.html) sections. There are lots of great tips and explanations in these documents.\n",
        "\n",
        "Weâ€™ll cover six key components that we can tune:\n",
        "\n",
        "1. Embeddings â€“ how we represent sentences numerically\n",
        "\n",
        "2. Dimensionality Reduction â€“ reducing the complexity of the embeddings\n",
        "\n",
        "3. Clustering â€“ grouping similar texts\n",
        "\n",
        "4. Tokenization â€“ how we split text into pieces\n",
        "\n",
        "5. Weighting Scheme â€“ how we decide which words matter most\n",
        "\n",
        "6. Topic Representation â€“ how we name/describe each topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy0LpM6KMYGj"
      },
      "source": [
        "#### Step 2: Prepare the Data and Set Up Batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smQwgQ4xWxr9"
      },
      "source": [
        "We'll now prepare the tweet dataset, sort it by time, and break it into smaller batches. This is useful when dealing with large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Output directory\n",
        "output_dir = \"output_path\"\n",
        "results_dir = os.path.join(output_dir, \"results\")\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# STEP 0 â€” Filter the tweets using the regex pattern\n",
        "df_filtered = df_sample[df_sample[\"text_clean\"].str.contains(regex_pattern, case=False, na=False)]\n",
        "\n",
        "# Ensure the 'timestamp' column is in datetime format\n",
        "df_filtered['timestamp'] = pd.to_datetime(df_filtered['timestamp'], errors='coerce')\n",
        "\n",
        "# Drop rows with invalid or missing timestamps\n",
        "df_filtered = df_filtered.dropna(subset=['timestamp'])\n",
        "\n",
        "# Filter the data to include only rows within the desired timeframe\n",
        "start_date = '2018-01-01'\n",
        "end_date = '2022-12-31'\n",
        "df_filtered = df_filtered[(df_filtered['timestamp'] >= start_date) & (df_filtered['timestamp'] <= end_date)]\n",
        "\n",
        "# Sort the filtered tweet dataset by time\n",
        "df_filtered = df_filtered.sort_values(by=\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "# Extract the text and timestamps for further processing\n",
        "all_docs = df_filtered[\"text_clean\"].dropna().tolist()\n",
        "all_dates = df_filtered[\"timestamp\"].tolist()  # for time range logging\n",
        "\n",
        "# Config\n",
        "total_docs = len(all_docs)\n",
        "batch_size = 10000\n",
        "nr_batches = int(np.ceil(total_docs / batch_size))\n",
        "\n",
        "print(f\"ğŸ“¦ Total tweets (regex-matched and within timeframe): {total_docs}\")\n",
        "print(f\"ğŸ“¦ Total batches: {nr_batches}\")\n",
        "print(f\"ğŸ“… Timeframe of the filtered data: {df_filtered['timestamp'].min()} to {df_filtered['timestamp'].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Output directory\n",
        "output_dir = \"output_path\"\n",
        "results_dir = os.path.join(output_dir, \"results\")\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# STEP 0 â€” Filter the tweets using the regex pattern\n",
        "df_filtered = df_sample[df_sample[\"text_clean\"].str.contains(regex_pattern, case=False, na=False)]\n",
        "\n",
        "# Ensure the 'timestamp' column is in datetime format\n",
        "df_filtered['timestamp'] = pd.to_datetime(df_filtered['timestamp'], errors='coerce')\n",
        "\n",
        "# Drop rows with invalid or missing timestamps\n",
        "df_filtered = df_filtered.dropna(subset=['timestamp'])\n",
        "\n",
        "# Filter the data to include only rows within the desired timeframe\n",
        "start_date = '2018-01-01'\n",
        "end_date = '2022-12-31'\n",
        "df_filtered = df_filtered[(df_filtered['timestamp'] >= start_date) & (df_filtered['timestamp'] <= end_date)]\n",
        "\n",
        "# Sort the filtered tweet dataset by time\n",
        "df_filtered = df_filtered.sort_values(by=\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "# Extract the text and timestamps for further processing\n",
        "all_docs = df_filtered[\"text_clean\"].dropna().tolist()\n",
        "all_dates = df_filtered[\"timestamp\"].tolist()  # for time range logging\n",
        "\n",
        "# Config\n",
        "total_docs = len(all_docs)\n",
        "batch_size = 10000\n",
        "nr_batches = int(np.ceil(total_docs / batch_size))\n",
        "\n",
        "print(f\"ğŸ“¦ Total tweets (regex-matched and within timeframe): {total_docs}\")\n",
        "print(f\"ğŸ“¦ Total batches: {nr_batches}\")\n",
        "print(f\"ğŸ“… Timeframe of the filtered data: {df_filtered['timestamp'].min()} to {df_filtered['timestamp'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V89h92QPW2cu"
      },
      "source": [
        "#### Step 3: Customize BERTopic Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9F1nrMCGxuW"
      },
      "source": [
        "Here we fine-tune different components of BERTopic. Each one plays a specific role in how topics are discovered:\n",
        "\n",
        "**Tunable parameters in UMAP**\n",
        "\n",
        "N_components- number of dimensions to reduce. (default:5)\n",
        "- Lower values may lose information but improve clustering efficiency.\n",
        "- Higher values retain more details but may make clustering harder\n",
        "\n",
        "N_neighbours- number of nearest neighbours considered. (Default:15)\n",
        "- Lower values emphasize local structures(useful for fine-grained topics).\n",
        "- Higher values emphasize global structures (useful for broad, general topics).\n",
        "\n",
        "Min_dist- minimum distance between points in a reduced space\n",
        "- Lower values create dense cluster\n",
        "- Higher values spread clusters apart\n",
        "\n",
        "\n",
        "**Tunable parameters in Clustering- HDBSCAN**\n",
        "\n",
        "Min_cluster_size- the minimum documents per cluster\n",
        "- Lower values allow small topics to emerge\n",
        "- Larger values force larger, general topics\n",
        "\n",
        "Min_samples- controls how strictly a point belongs to a cluster\n",
        "- Lower values lead to more clusters\n",
        "- Higher topics remove noise but may merge distinct topics\n",
        "\n",
        "Cluster_selection_method- determines how topics are framed\n",
        "- â€œeomâ€(Excess of Mass- default)- finds dense clusters\n",
        "- â€œLeafâ€- allows more topic granularity\n",
        "\n",
        "\n",
        "**K-MEANS**\n",
        "\n",
        "N_clusters- forces the exact number of topics\n",
        "- Lower amount of clusters makes them broad and mixed\n",
        "- Larger amount of clusters make them too specific\n",
        "\n",
        "**Tokenizing**\n",
        "\n",
        "Adjusting stopwords\n",
        "helps remove uninformative words, but be careful not to exclude meaningful ones.\n",
        "\n",
        "min_df\n",
        "\n",
        "- Increasing filters out more rare terms, reducing noise.\n",
        "- Decreasing includes more rare terms, increasing detail but also noise.\n",
        "\n",
        "ngram_range\n",
        "expanding captures more context (e.g., adding trigrams), but increases dimensionality.\n",
        "\n",
        "**Weighting Scheme**\n",
        "\n",
        "Reducing frequent words: True\n",
        "- Downweights common words across topics, making topics more distinct and keywords more meaningful.\n",
        "False\n",
        "- Treats all words equally, so frequent terms might dominate and reduce topic clarity.\n",
        "\n",
        "\n",
        "**Representation Tuning**\n",
        "\n",
        "KeyBERT() â€“ Extracts contextually relevant keywords using BERT embeddings for meaningful topic representation.\n",
        "\n",
        "MaximalMarginalRelevance() â€“ Balances relevance and diversity of keywords.\n",
        "\n",
        "Simple() â€“ Returns top keywords by c-TF-IDF score (fast but basic).\n",
        "\n",
        "PartiallyRedundant() â€“ Allows some repetition while maintaining clarity.\n",
        "\n",
        "MMR() â€“ Another name for MaximalMarginalRelevance().\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from umap import UMAP\n",
        "from sklearn.cluster import KMeans\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "\n",
        "# Download Dutch stopwords\n",
        "nltk.download('stopwords')\n",
        "dutch_stopwords = stopwords.words('dutch')\n",
        "\n",
        "# Load custom stopwords from a file\n",
        "custom_stopwords_file = \"stopword_file\"  # Replace with the path to your file\n",
        "with open(custom_stopwords_file, \"r\") as file:\n",
        "    custom_stop_words = [line.strip() for line in file.readlines()]\n",
        "\n",
        "# Combine Dutch stopwords and custom stopwords\n",
        "combined_stop_words = list(set(dutch_stopwords + custom_stop_words))\n",
        "\n",
        "# Print the first few stopwords to verify\n",
        "print(\"Combined Stopwords:\", combined_stop_words[:10])\n",
        "\n",
        "# Define models\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=10)\n",
        "hdbscan_model = KMeans(n_clusters=30)\n",
        "vectorizer_model = CountVectorizer(stop_words=combined_stop_words, ngram_range=(1, 2), min_df=5)\n",
        "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
        "representation_model = KeyBERTInspired()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4: Run BERTopic in the First Batch and Test the Paramesters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we run the BERTopic in all batches, let's first try the paramesters with batch 0. This is very important. Running the whole dataset consumes lots of time. So let's first test and find a suitable parameter in Step 4. Then, we can run all the batches in Step 5. \n",
        "\n",
        "This Step usually takes more efforts and time than other Steps. Remeber to be patient :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Setup for Batch 1 ---\n",
        "batch = 0\n",
        "batch_start = batch * batch_size\n",
        "batch_end = min((batch + 1) * batch_size, total_docs)\n",
        "\n",
        "docs = all_docs[batch_start:batch_end]\n",
        "dates = all_dates[batch_start:batch_end]\n",
        "\n",
        "print(f\"\\nğŸš€ [TEST RUN] Starting batch {batch + 1}/{nr_batches} â€” {len(docs)} tweets\")\n",
        "\n",
        "# Time range info\n",
        "start_time_str = pd.to_datetime(dates[0]).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "end_time_str = pd.to_datetime(dates[-1]).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "print(f\"ğŸ•’ Batch {batch + 1} covers: {start_time_str} â†’ {end_time_str}\")\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = sentence_model.encode(docs, batch_size=48, show_progress_bar=True)\n",
        "\n",
        "# Initialize BERTopic model\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=sentence_model,\n",
        "    ctfidf_model=ctfidf_model,\n",
        "    representation_model=representation_model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Run topic modeling\n",
        "topics, probs = topic_model.fit_transform(docs, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you have get the result of batch 0. Let's check the topics below. \n",
        "\n",
        "If the topics are not good, you can adjust the parameters above and rerun it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get topic info\n",
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also visualize the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.visualize_documents(\n",
        "    docs=docs,\n",
        "    embeddings = sentence_model.encode(docs, batch_size=48, show_progress_bar=True),\n",
        "    hide_annotations=True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once you find suitable parameters for your data, you can go head to the Automatic Batch Runner!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piN_qf0aW8FR"
      },
      "source": [
        "#### Step 5: Run BERTopic in Batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PEKW7vhMtJu"
      },
      "source": [
        "This loop processes the data in chunks. For each batch of tweets, we:\n",
        "\n",
        "Generate sentence embeddings\n",
        "\n",
        "Run BERTopic with all the custom components\n",
        "\n",
        "Save the topic information and per-document results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuZwnn_kkXmT"
      },
      "outputs": [],
      "source": [
        "for batch in tqdm(range(nr_batches), desc=\"ğŸ” Processing Batches\", unit=\"batch\"):\n",
        "    batch_start = batch * batch_size\n",
        "    batch_end = min((batch + 1) * batch_size, total_docs)\n",
        "\n",
        "    docs = all_docs[batch_start:batch_end]\n",
        "    dates = all_dates[batch_start:batch_end]\n",
        "\n",
        "    batch_str = str(batch).zfill(2)\n",
        "    print(f\"\\nğŸš€ Starting batch {batch + 1}/{nr_batches} â€” {len(docs)} tweets\")\n",
        "\n",
        "    # Time range for this batch\n",
        "    start_time_str = pd.to_datetime(dates[0]).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    end_time_str = pd.to_datetime(dates[-1]).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"ğŸ•’ Batch {batch + 1} covers: {start_time_str} â†’ {end_time_str}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = sentence_model.encode(docs, batch_size=48, show_progress_bar=True)\n",
        "\n",
        "    # Run BERTopic\n",
        "    topic_model = BERTopic(\n",
        "        embedding_model=sentence_model,\n",
        "        ctfidf_model=ctfidf_model,\n",
        "        representation_model=representation_model,\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        vectorizer_model=vectorizer_model,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    topics, probs = topic_model.fit_transform(docs, embeddings)\n",
        "\n",
        "    # Save outputs\n",
        "    documents = topic_model.get_document_info(docs)\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "\n",
        "    documents.to_csv(os.path.join(results_dir, f\"twitter_batch{batch_str}_documents.csv\"), sep=\";\")\n",
        "    topic_info.to_csv(os.path.join(results_dir, f\"twitter_batch{batch_str}_topic_info.csv\"), sep=\";\")\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    print(f\"âœ… Finished batch {batch + 1}/{nr_batches} in {duration:.2f} seconds\")\n",
        "    print(f\"ğŸ“ Results saved to: twitter_batch{batch_str}_*.csv\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f5BOLpfN7su"
      },
      "source": [
        "Now we have finished all the batch running. Let's then check the topic_info of batch 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyyDS1SnRHVq"
      },
      "outputs": [],
      "source": [
        "topic_info_b01 = pd.read_csv(os.path.join(results_dir, \"twitter_batch01_topic_info.csv\"), sep=';')\n",
        "topic_info_b01.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18F8fjSnSH2S"
      },
      "source": [
        "If everything is correct, let's then merge all the output document in the following section. Or you might want to adjust some parameters or stopwords above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HfF1NYXkXmT"
      },
      "source": [
        "### Merge All The Outputs\n",
        "\n",
        "Now we have got all the results of each batch. For overall analysis, we need to merge all the topics and documents together seperately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmLtixk7lygO"
      },
      "source": [
        "#### Step 1: Merge Top 10 Topics in Each Batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7DlytMDo2n2"
      },
      "source": [
        "We first select top 10 topics of each batch and merge them together in a dataframe. It helps us to get the main topics across all the batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB4HSKTnlxgO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def merge_top_topics(results_dir, top_n=10): # you can change the number of topics you want here\n",
        "    \"\"\"\n",
        "    Merges the top N topics from each batch's topic_info file.\n",
        "\n",
        "    Args:\n",
        "        results_dir: The directory containing the batch results.\n",
        "        top_n: The number of top topics to consider from each batch.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing the merged top topics.\n",
        "    \"\"\"\n",
        "    all_top_topics = []\n",
        "    for filename in os.listdir(results_dir):\n",
        "        if filename.startswith(\"twitter_batch\") and filename.endswith(\"_topic_info.csv\"):\n",
        "            filepath = os.path.join(results_dir, filename)\n",
        "            try:\n",
        "                df = pd.read_csv(filepath, sep=';')\n",
        "                # Exclude the outlier topic (-1)\n",
        "                top_topics_df = df[df[\"Topic\"] != -1].nlargest(top_n, \"Count\")\n",
        "                all_top_topics.append(top_topics_df)\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Warning: Skipping empty file: {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    if all_top_topics:\n",
        "        merged_df = pd.concat(all_top_topics, ignore_index=True)\n",
        "        return merged_df\n",
        "    else:\n",
        "        print(\"No valid topic info files found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "merged_topics = merge_top_topics(results_dir)\n",
        "\n",
        "if not merged_topics.empty:\n",
        "    merged_topics.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixBGEDefpSBm"
      },
      "source": [
        "Then, we convert this dataframe to csv file and save it in local directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-oiXbJIooMA"
      },
      "outputs": [],
      "source": [
        "# Save to CSV if the DataFrame is not empty\n",
        "if not merged_topics.empty:\n",
        "    output_path = os.path.join(results_dir, \"merged_top_topics.csv\")\n",
        "    merged_topics.to_csv(output_path, sep=\";\", index=False)\n",
        "    print(f\"âœ… Merged top topics saved to: {output_path}\")\n",
        "else:\n",
        "    print(\"âš ï¸ Skipped saving: merged_topics is empty.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OUH36WCp39T"
      },
      "source": [
        "Now you have got the csv file of merged top 10 topics! You can go to spreasheet to check the result and do manual labelling to assign them into larger catergories.\n",
        "\n",
        "When you finish the manual labeling, you can assign them to merged document dataframe in Step 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub7BChcZT7PU"
      },
      "source": [
        "#### Step 2: Merge Documents of Batches\n",
        "\n",
        "We also want to merge the document results into a single file.\n",
        "\n",
        "This step ensures that we can analyze the full set of topic assignments across all tweets at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XfpHOiykXmT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load all the document CSV file in the output directory\n",
        "directory = results_dir\n",
        "pattern = re.compile(r\"twitter_batch(\\d+)_documents\\.csv\")\n",
        "\n",
        "files_with_index = []\n",
        "for filename in os.listdir(directory):\n",
        "    match = pattern.match(filename)\n",
        "    if match:\n",
        "        index = int(match.group(1))\n",
        "        files_with_index.append((index, filename))\n",
        "\n",
        "files_with_index.sort()\n",
        "\n",
        "dfs = []\n",
        "for index, filename in files_with_index:\n",
        "    path = os.path.join(directory, filename)\n",
        "    try:\n",
        "        # âœ… Use semicolon separator!\n",
        "        df = pd.read_csv(path, sep=\";\")\n",
        "\n",
        "        # Drop index column if present (was likely saved earlier)\n",
        "        if \"Unnamed: 0\" in df.columns:\n",
        "            df = df.drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "        # Optional: track the batch source\n",
        "        df[\"batch_index\"] = index\n",
        "\n",
        "        dfs.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Failed to load {filename}: {e}\")\n",
        "\n",
        "# Merge and save\n",
        "if dfs:\n",
        "    merged_df = pd.concat(dfs, ignore_index=True)\n",
        "    output_file = os.path.join(directory, \"twitter_merged_documents.csv\")\n",
        "    merged_df.to_csv(output_file, sep=\";\", index=False)  # âœ… Save with same semicolon format\n",
        "    print(f\"âœ… Merged {len(files_with_index)} files into {output_file}\")\n",
        "else:\n",
        "    print(\"âŒ No files were loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGL9U6HzkXmT"
      },
      "source": [
        "After saving the merged dataframe into csv file, we can load it to check whether there is any mistake in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9G1hlXlkXmT"
      },
      "outputs": [],
      "source": [
        "# Load the merged CSV file into a DataFrame\n",
        "merged_path = os.path.join(results_dir, \"twitter_merged_documents.csv\")\n",
        "merged_df = pd.read_csv(merged_path, sep=';')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "merged_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff9yAfjEkXmU"
      },
      "source": [
        "#### Step 3: Assign timestamp from original dataset to the merged document dataframe\n",
        "\n",
        "The result of topic modeling ususally does not include a timestamp of each topic and document. While it is very important to do time series analysis in cross-media research, such as analyzing the topic trend across time, topic composition and so on. We then go back to the cleaned data we have before BERTopic and try to assign the timestamp from it to our merged result document df."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm_GCcqMxvS9"
      },
      "source": [
        "Let's first check if there are same content in \"text_clean\" column of cleaned data as the content in \"Document\" column of merged document df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGINFlGlkXmU",
        "outputId": "64e2f9ed-aa4d-493e-860b-a5b18b5cb206"
      },
      "outputs": [],
      "source": [
        "# How many Document entries have an exact match in text_clean?\n",
        "matches = merged_df[\"Document\"].isin(df_sample[\"text_clean\"])\n",
        "print(f\"âœ… Matched {matches.sum()} out of {len(merged_df)} documents ({matches.mean() * 100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFEeplnTyapF"
      },
      "source": [
        "Once we find them match each other, we can assign the timestamp in the merged document dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKaRGzqCkXmU",
        "outputId": "1ef4acff-a53d-4c69-9b93-d4a3cfe0b1ba"
      },
      "outputs": [],
      "source": [
        "# Merge in timestamp\n",
        "final_df = pd.merge(\n",
        "    merged_df,\n",
        "    df_sample[['text_clean', 'timestamp']],\n",
        "    left_on='Document',\n",
        "    right_on='text_clean',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Optional cleanup: drop redundant text_clean column\n",
        "final_df.drop(columns=['text_clean'], inplace=True)\n",
        "\n",
        "# Save for visualization\n",
        "final_df.to_csv(\"bertopic_output_with_timestamp.csv\", sep=\";\", index=False)\n",
        "\n",
        "print(\"âœ… Timestamp added successfully and data saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hiw6xjklyvfe"
      },
      "source": [
        "Let's check the merged dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQOI06OLkXmU",
        "outputId": "71ce1db3-77b2-4aa2-f39c-d0b275ae5f2c"
      },
      "outputs": [],
      "source": [
        "final_df.info()\n",
        "final_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBPxO02zy-Ti"
      },
      "source": [
        "Good! You have assigned the timestamp to the merged document dataframe. Now we only need one last step - assign the custom label you have finished after step 1 to the new dataframe with timestamp. Then you are ready for visualization!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkrVKchbrUgT"
      },
      "source": [
        "#### Step 4: Merge the top 10 topics' custom label to the merged document dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91f3HwZTrkgc"
      },
      "source": [
        "Once you finish the manual label after Step 1, let's load the csv with custom label back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFoC7rYnkXmU",
        "outputId": "f21c333d-a0f1-408f-a632-01b4e7d87867"
      },
      "outputs": [],
      "source": [
        "df_label = pd.read_csv('path_to step1_file', sep=',') # change to your own path and file with custom label here\n",
        "df_label.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-iunNFKud_3"
      },
      "source": [
        "Then, we assign the custom label to the data if their batch and topics in two different dataframes both match each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEcKqNt-kXmU",
        "outputId": "f3e492f9-fcd3-4093-8f52-1bab1766a6aa"
      },
      "outputs": [],
      "source": [
        "# Step 1: Merge on both batch and topic\n",
        "df_merged_labeled = pd.merge(\n",
        "    final_df,\n",
        "    df_label[['Batch', 'Topic', 'Custom Label']],\n",
        "    left_on=['batch_index', 'Topic'],\n",
        "    right_on=['Batch', 'Topic'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Step 2: Fill missing labels with 'Other Topics'\n",
        "df_merged_labeled['Custom Label'] = df_merged_labeled['Custom Label'].fillna('Other Topics')\n",
        "\n",
        "# Step 3: Optional cleanup (drop 'batch' column from df_label)\n",
        "#df_merged_labeled = df_merged_labeled.drop(columns=['batch'])\n",
        "\n",
        "# âœ… Done\n",
        "print(\"âœ… Custom labels merged successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpf0FF6VkXmU",
        "outputId": "44e79ce5-e55e-4760-b44b-615814e623db"
      },
      "outputs": [],
      "source": [
        "df_merged_labeled.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS03svVZu9L3"
      },
      "source": [
        "Nice! Now we have assign all the tweets with custom label. While this dataframe also includes \"other topics\", which we would not use for the visualization and future analysis. Let's then filter out \"other topics\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0yX6UYfkXmU"
      },
      "outputs": [],
      "source": [
        "df_merged_labeled.to_csv('path _to_labelleddata', sep=';', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tliphz2kXmU",
        "outputId": "2726f518-3dfa-4162-87bf-02ed1d0b5bb6"
      },
      "outputs": [],
      "source": [
        "# Filter out rows labeled as \"Other Topics\"\n",
        "top10_document = df_merged_labeled[df_merged_labeled[\"Custom Label\"] != \"Other Topics\"]\n",
        "\n",
        "top10_document.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHT4zS_RkXmV",
        "outputId": "63e40ce4-1813-4732-d44f-5c34825ce26a"
      },
      "outputs": [],
      "source": [
        "# Save the filtered DataFrame to a new CSV file\n",
        "top10_document.to_csv('output_path.csv', sep=';', index=False)\n",
        "\n",
        "print(\"âœ… Filtered data saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M0HTlN3kXmV"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoJPVPGyv2_M"
      },
      "source": [
        "Now let's visualize the main topics. Here we offer three visualizations often used by cross-media scholars, inlcuding topic over time (streamgraph), topic composition over time (streamgraph) and topic keyword visualization (bubble chart)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGWigqRTtif6"
      },
      "source": [
        "#### Reload top10_document (optional)\n",
        "If you lost your session, you can reload your top10_document here for the following visualization tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKLPUkwXs-He"
      },
      "outputs": [],
      "source": [
        "top10_document = pd.read_csv('output_path.csv', sep=';')\n",
        "top10_document['timestamp'] = pd.to_datetime(top10_document['timestamp'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpGexbUmkXmV",
        "outputId": "c9015160-fe0e-4606-a84d-b68fdeb1dd33"
      },
      "outputs": [],
      "source": [
        "top10_document.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh7fpMNaxfWW"
      },
      "source": [
        "To keep all the color aligning with the same topics across different visualization, we first assign them together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dMIAWr5xtDG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate overall proportions for sorting\n",
        "label_proportions = top10_document['Custom Label'].value_counts(normalize=True)\n",
        "\n",
        "# Sort labels from highest to lowest overall proportion\n",
        "label_order = label_proportions.index.tolist()\n",
        "\n",
        "# Get colors from tab20c colormap in sorted order\n",
        "cmap = plt.get_cmap('tab20c')\n",
        "colors = [cmap(i) for i in range(len(label_order))]\n",
        "\n",
        "# Explicit color mapping: labels â†’ colors\n",
        "color_mapping = dict(zip(label_order, colors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQUpARQJkXmV"
      },
      "source": [
        "Then, we generate different visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oxhG7uXb3pQ"
      },
      "source": [
        "#### Topic Dynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMNEOLCjbXQU"
      },
      "source": [
        "**Streamgraph of Topic Trends**\n",
        "\n",
        "This visualization creates a streamgraph-style plot, it displays the raw weekly counts of each topic. This gives an accurate view of short-term spikes or bursts in topic frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbzCO6kikXmV"
      },
      "outputs": [],
      "source": [
        "# streamgraph\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure timestamp is datetime\n",
        "top10_document['timestamp'] = pd.to_datetime(top10_document['timestamp'])\n",
        "\n",
        "# Optional: aggregate to daily/weekly/monthly counts\n",
        "top10_document['date'] = top10_document['timestamp'].dt.to_period('W').dt.start_time\n",
        "\n",
        "# Pivot the table to wide format for streamgraph\n",
        "pivot_df = top10_document.groupby(['date', 'Custom Label']).size().unstack(fill_value=0)\n",
        "\n",
        "# Sort columns by total volume if needed\n",
        "pivot_df = pivot_df[pivot_df.sum().sort_values(ascending=False).index]\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "ax.stackplot(pivot_df.index, pivot_df.T.values, labels=pivot_df.columns)\n",
        "\n",
        "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "ax.set_title(\"Custom Topic Labels Over Time (Streamgraph Style)\", fontsize=16)\n",
        "ax.set_xlabel(\"Date\")\n",
        "ax.set_ylabel(\"Document Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFAi_p7ryVgR"
      },
      "source": [
        "**Streamgraph of Topic Trends (Smoothened)**\n",
        "\n",
        "This version creates a streamgraph-style plot showing how the top topics evolve over time, with smoothed lines to reduce noise and better highlight long-term trends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfdve1u6kXmV"
      },
      "outputs": [],
      "source": [
        "top10_document['date'] = top10_document['timestamp'].dt.to_period('W').dt.start_time\n",
        "pivot_weekly = top10_document.groupby(['date', 'Custom Label']).size().unstack(fill_value=0)\n",
        "\n",
        "# Sort columns explicitly\n",
        "pivot_weekly = pivot_weekly[label_order]\n",
        "\n",
        "pivot_smoothed = pivot_weekly.rolling(window=4, min_periods=1).mean()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "ax.stackplot(\n",
        "    pivot_smoothed.index,\n",
        "    pivot_smoothed.T.values,\n",
        "    labels=pivot_smoothed.columns,\n",
        "    colors=[color_mapping[label] for label in pivot_smoothed.columns],\n",
        "    baseline='sym'\n",
        ")\n",
        "\n",
        "ax.set_title(\"Twitter Topics Over Time\", fontsize=16)\n",
        "ax.set_xlabel(\"Date\")\n",
        "ax.set_ylabel(\"Tweet Count\")\n",
        "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFi76yOzkXmV"
      },
      "source": [
        "**Monthly Topic Composition (Smoothed Streamgraph by Percentage)**\n",
        "\n",
        "This plot shows how different topics (Custom Labels) contribute proportionally to the tweet volume each month, using a percentage-based streamgraph. Instead of showing raw tweet counts, this version highlights the relative importance or dominance of each topic over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKbKbN0nkXmV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure timestamp is datetime\n",
        "top10_document['timestamp'] = pd.to_datetime(top10_document['timestamp'])\n",
        "top10_document['month'] = top10_document['timestamp'].dt.to_period('M').dt.start_time\n",
        "\n",
        "# Aggregate data by month\n",
        "grouped = top10_document.groupby(['month', 'Custom Label']).size().reset_index(name='count')\n",
        "grouped['month_total'] = grouped.groupby('month')['count'].transform('sum')\n",
        "grouped['percentage'] = grouped['count'] / grouped['month_total'] * 100\n",
        "\n",
        "# Pivot data for plotting\n",
        "pivot_monthly = grouped.pivot(index='month', columns='Custom Label', values='percentage').fillna(0)\n",
        "\n",
        "# Sort columns explicitly\n",
        "pivot_monthly = pivot_monthly[label_order]\n",
        "\n",
        "# Apply smoothing (3-month rolling mean)\n",
        "pivot_smoothed = pivot_monthly.rolling(window=3, min_periods=1, center=True).mean()\n",
        "\n",
        "# Plotting the smoothed monthly streamgraph\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "ax.stackplot(\n",
        "    pivot_smoothed.index,\n",
        "    pivot_smoothed.T.values,\n",
        "    labels=pivot_smoothed.columns,\n",
        "    colors=[color_mapping[label] for label in pivot_smoothed.columns],\n",
        "    baseline='sym'\n",
        ")\n",
        "\n",
        "# Formatting\n",
        "ax.set_title(\"Twitter Topic Composition Over Time\", fontsize=16)\n",
        "ax.set_xlabel(\"Time\")\n",
        "ax.set_ylabel(\"Topic Share (%)\")\n",
        "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIVOm1p2b8DY"
      },
      "source": [
        "#### Topic Keyword Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21FLQqBBkpFL"
      },
      "source": [
        "Let's also generate a Keyword Clustered Bubble Chart to see the keywords within each topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktDAVC3RkzdF"
      },
      "outputs": [],
      "source": [
        "# we first need to select top10 keywords of each topic\n",
        "# Create a new DataFrame by dropping the specified columns\n",
        "top10_document_keywords = top10_document.drop(columns=['Document','Name','Representative_Docs','Representative_document','timestamp', 'Batch', 'week', 'month', 'date'])\n",
        "#remove duplicate data\n",
        "top10_document_keywords = top10_document_keywords.drop_duplicates()\n",
        "# Display the first few rows of the new DataFrame\n",
        "top10_document_keywords.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_UwnBeMY0sQ"
      },
      "source": [
        "Then, let's count the frequency of each words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yS7eE-YfzWtf"
      },
      "outputs": [],
      "source": [
        "# Create a new column to count the frequency of each keyword within topics\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Merge all 'representation' together if they have the same 'custom label'\n",
        "merged_representations = (\n",
        "    top10_document_keywords.groupby('Custom Label')['Representation']\n",
        "    .apply(lambda x: ' '.join(x))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Step 2: Count the number of each keyword in 'representation'\n",
        "keyword_counts = []\n",
        "for _, row in merged_representations.iterrows():\n",
        "    custom_label = row['Custom Label']\n",
        "    representation = row['Representation']\n",
        "    keywords = representation.replace('[', '').replace(']', '').replace(\"'\", '').split(', ')\n",
        "    keyword_counter = Counter(keywords)\n",
        "    for keyword, count in keyword_counter.items():\n",
        "        keyword_counts.append({'Custom Label': custom_label, 'Keyword': keyword, 'Count': count})\n",
        "\n",
        "# Step 3: Create a new dataframe\n",
        "keyword_df = pd.DataFrame(keyword_counts)\n",
        "\n",
        "# Group by 'Custom Label' and 'Keyword' and sum their counts\n",
        "combined_keyword_counts = keyword_df.groupby(['Custom Label', 'Keyword'], as_index=False)['Count'].sum()\n",
        "\n",
        "# Group by 'Custom Label' and aggregate the keywords with their counts\n",
        "top_keywords_per_label = (\n",
        "    keyword_df.groupby('Custom Label')\n",
        "    .apply(lambda group: group.nlargest(10, 'Count'))\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "top_keywords_per_label.head(20)  # Display the first 20 rows for verification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTAB3DkaY6RR"
      },
      "source": [
        "Optional: if you need to add English translation column of keywords, you can save them as csv file first and edit in the spreadsheet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgQhnxObzvwl"
      },
      "outputs": [],
      "source": [
        "# Save the top_keywords_per_label DataFrame to a CSV file in case you need to add a column of translation of these keywords\n",
        "top_keywords_per_label.to_csv('output_path_top10.csv', index=False)\n",
        "\n",
        "print(\"âœ… top_keywords_per_label saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oqpa5cj0LCZ"
      },
      "source": [
        "When you finish the English column, load the new dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTuNinbE0JgQ"
      },
      "outputs": [],
      "source": [
        "top10_document_en = pd.read_csv('output_path_translation', sep=',')\n",
        "top10_document_en.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ40ftwO0jBp"
      },
      "source": [
        "If you need an English version, you can use the following code to generate. This code also ensure all the labels show on the visualization. While if some of the keywords are too long, they might overlap each other. If you want to use English keywords directly, you can simply change some parts following the annotations (###)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1BdbcTq0cbG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load and prepare your data\n",
        "data = top10_document_en[['Keyword', 'Count', 'Custom Label']].copy() # Dutch keywords\n",
        "### for English keywords, using the following one.\n",
        "# data = top10_document_en[['Keyword(EN)', 'Count', 'Custom Label']].copy() # English keywords\n",
        "data['Radius'] = np.sqrt(data['Count']) * 0.2\n",
        "\n",
        "# Assign cluster colors\n",
        "labels = data['Custom Label'].unique().tolist()\n",
        "cmap = plt.get_cmap(\"tab20c\")\n",
        "label_colors = {label: cmap(i) for i, label in enumerate(labels)}\n",
        "data['Color'] = data['Custom Label'].map(label_colors)\n",
        "\n",
        "# Initialize positions (wider than tall)\n",
        "np.random.seed(42)\n",
        "data['x'] = np.random.uniform(-20, 20, size=len(data))\n",
        "data['y'] = np.random.uniform(-10, 10, size=len(data))\n",
        "\n",
        "# --- layout with no overlap and strong clustering ---\n",
        "def resolve_bubble_forces(df, steps=700, repel_strength=0.12, attract_strength=0.02, padding=0.18):\n",
        "    for _ in range(steps):\n",
        "        dx = np.zeros(len(df))\n",
        "        dy = np.zeros(len(df))\n",
        "        for i in range(len(df)):\n",
        "            xi, yi, ri, li = df.iloc[i][['x', 'y', 'Radius', 'Custom Label']]\n",
        "            for j in range(i + 1, len(df)):\n",
        "                xj, yj, rj, lj = df.iloc[j][['x', 'y', 'Radius', 'Custom Label']]\n",
        "                dx_ij = xi - xj\n",
        "                dy_ij = yi - yj\n",
        "                dist = np.hypot(dx_ij, dy_ij)\n",
        "                min_dist = ri + rj + padding\n",
        "                if dist < min_dist and dist > 0:\n",
        "                    force = (min_dist - dist) * repel_strength\n",
        "                    dx_i = (dx_ij / dist) * force\n",
        "                    dy_i = (dy_ij / dist) * force\n",
        "                    dx[i] += dx_i\n",
        "                    dy[i] += dy_i\n",
        "                    dx[j] -= dx_i\n",
        "                    dy[j] -= dy_i\n",
        "\n",
        "            # Stronger pull to cluster center\n",
        "            cluster_center_x = df[df['Custom Label'] == li]['x'].mean()\n",
        "            cluster_center_y = df[df['Custom Label'] == li]['y'].mean()\n",
        "            dx[i] += (cluster_center_x - xi) * attract_strength\n",
        "            dy[i] += (cluster_center_y - yi) * attract_strength\n",
        "\n",
        "        df['x'] += dx\n",
        "        df['y'] += dy\n",
        "    return df\n",
        "\n",
        "# Apply the simulation\n",
        "data = resolve_bubble_forces(data)\n",
        "\n",
        "# --- Plot ---\n",
        "fig, ax = plt.subplots(figsize=(18, 12))  # wider than tall\n",
        "ax.set_aspect('equal')\n",
        "ax.axis('off')\n",
        "\n",
        "# Draw each bubble\n",
        "for _, row in data.iterrows():\n",
        "    x, y, r = row['x'], row['y'], row['Radius']\n",
        "    color = row['Color']\n",
        "    label = row['Keyword'] # Dutch Keywords\n",
        "    #label = row['Keyword(EN)'] ### English Keywords here\n",
        "    circle = plt.Circle((x, y), r, color=color, alpha=0.75, ec='black', lw=0.5)\n",
        "    ax.add_patch(circle)\n",
        "\n",
        "    # Show label on all bubbles (scaled)\n",
        "    fontsize = max(6, r * 4 + 4)\n",
        "    ax.text(x, y, label, fontsize=fontsize, ha='center', va='center')\n",
        "\n",
        "# Add cluster legend\n",
        "legend_handles = [mpatches.Patch(color=label_colors[label], label=label) for label in labels]\n",
        "ax.legend(handles=legend_handles, title=\"Custom Label\", bbox_to_anchor=(1.02, 1), loc='upper left', frameon=False)\n",
        "\n",
        "# Set plot bounds\n",
        "margin = 2\n",
        "ax.set_xlim(data['x'].min() - margin, data['x'].max() + margin)\n",
        "ax.set_ylim(data['y'].min() - margin, data['y'].max() + margin)\n",
        "\n",
        "plt.title(\"Clustered Packed Bubble Chart\", fontsize=18) ### Don't for get to change your title when you use English keywords\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1azt2Ev1C4v"
      },
      "source": [
        "Or you can also use the following one to only show the dominant keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGmxDcANkxER"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare data\n",
        "data = top10_document_en[['Keyword', 'Count', 'Custom Label']].copy() # Dutch keywords\n",
        "#data = top10_document_en[['Keyword(EN)', 'Count', 'Custom Label']].copy() ### English keywords\n",
        "data['Radius'] = np.sqrt(data['Count']) * 0.22\n",
        "\n",
        "# Assign color per cluster\n",
        "labels = data['Custom Label'].unique().tolist()\n",
        "cmap = plt.get_cmap(\"tab20c\")\n",
        "label_colors = {label: cmap(i) for i, label in enumerate(labels)}\n",
        "data['Color'] = data['Custom Label'].map(label_colors)\n",
        "\n",
        "# Initial positions\n",
        "np.random.seed(42)\n",
        "data['x'] = np.random.uniform(-30, 30, size=len(data))\n",
        "data['y'] = np.random.uniform(-10, 10, size=len(data))\n",
        "\n",
        "# Force-based layout\n",
        "def resolve_bubble_forces(df, steps=300, repel_strength=0.2, attract_strength=0.03, padding=0.35):\n",
        "    for _ in range(steps):\n",
        "        dx = np.zeros(len(df))\n",
        "        dy = np.zeros(len(df))\n",
        "        for i in range(len(df)):\n",
        "            xi, yi, ri, li = df.iloc[i][['x', 'y', 'Radius', 'Custom Label']]\n",
        "            for j in range(i + 1, len(df)):\n",
        "                xj, yj, rj, lj = df.iloc[j][['x', 'y', 'Radius', 'Custom Label']]\n",
        "                dx_ij = xi - xj\n",
        "                dy_ij = yi - yj\n",
        "                dist = np.hypot(dx_ij, dy_ij)\n",
        "                min_dist = ri + rj + padding\n",
        "                if dist < min_dist and dist > 0:\n",
        "                    force = (min_dist - dist) * repel_strength\n",
        "                    dx_i = (dx_ij / dist) * force\n",
        "                    dy_i = (dy_ij / dist) * force\n",
        "                    dx[i] += dx_i\n",
        "                    dy[i] += dy_i\n",
        "                    dx[j] -= dx_i\n",
        "                    dy[j] -= dy_i\n",
        "\n",
        "            # Cluster cohesion\n",
        "            cx = df[df['Custom Label'] == li]['x'].mean()\n",
        "            cy = df[df['Custom Label'] == li]['y'].mean()\n",
        "            dx[i] += (cx - xi) * attract_strength\n",
        "            dy[i] += (cy - yi) * attract_strength\n",
        "\n",
        "        df['x'] += dx\n",
        "        df['y'] += dy\n",
        "    return df\n",
        "\n",
        "# Apply layout\n",
        "data = resolve_bubble_forces(data)\n",
        "\n",
        "# --- PLOT ---\n",
        "fig, ax = plt.subplots(figsize=(20, 12))\n",
        "ax.set_aspect('equal')\n",
        "ax.axis('off')\n",
        "\n",
        "# Preload font sizes to fit labels\n",
        "def will_fit(ax, label, x, y, r, fontsize):\n",
        "    \"\"\"Return True if label fits within the circle of radius r.\"\"\"\n",
        "    t = ax.text(x, y, label, fontsize=fontsize, ha='center', va='center')\n",
        "    fig.canvas.draw()\n",
        "    bbox = t.get_window_extent()\n",
        "    t.remove()\n",
        "    pixel_diameter = 2 * r * fig.dpi  # estimate circle size in pixels\n",
        "    label_width = bbox.width\n",
        "    return label_width <= pixel_diameter * 0.9\n",
        "\n",
        "# Draw bubbles and check label fit\n",
        "for _, row in data.iterrows():\n",
        "    x, y, r = row['x'], row['y'], row['Radius']\n",
        "    color = row['Color']\n",
        "    label = row['Keyword'] # Dutch keywords\n",
        "    #label = row['Keyword(EN)'] ### English Keywords here\n",
        "    circle = plt.Circle((x, y), r, color=color, alpha=0.75, ec='black', lw=0.5)\n",
        "    ax.add_patch(circle)\n",
        "\n",
        "    fontsize = max(6, r * 4 + 2)\n",
        "    if will_fit(ax, label, x, y, r, fontsize):\n",
        "        ax.text(x, y, label, fontsize=fontsize, ha='center', va='center')\n",
        "\n",
        "# Legend\n",
        "legend_handles = [mpatches.Patch(color=label_colors[label], label=label) for label in labels]\n",
        "ax.legend(handles=legend_handles, title=\"Custom Label\", bbox_to_anchor=(1.02, 1), loc='upper left', frameon=False)\n",
        "\n",
        "# Set bounds\n",
        "margin = 2\n",
        "ax.set_xlim(data['x'].min() - margin, data['x'].max() + margin)\n",
        "ax.set_ylim(data['y'].min() - margin, data['y'].max() + margin)\n",
        "\n",
        "plt.title(\"Keyword Clustered Bubble Chart\", fontsize=18) ### Don't for get to change your title when you use English keywords\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validity and Reliability of BERTopic\n",
        "### 1. Topic Intrusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.save(\"topic_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Step 1: Verify the model file path\n",
        "model_path = \"topic_model\"  # Replace with the correct path to your saved model\n",
        "if not os.path.exists(model_path):\n",
        "    raise FileNotFoundError(f\"The model file '{model_path}' does not exist. Please check the path.\")\n",
        "\n",
        "# Step 2: Load the BERTopic model\n",
        "topic_model = BERTopic.load(model_path)\n",
        "\n",
        "# Step 3: Get top words per topic\n",
        "topic_words = topic_model.get_topics()\n",
        "\n",
        "# Number of top words to display per topic (excluding the intruder)\n",
        "top_n = 4\n",
        "\n",
        "# Step 4: Define a function to create word intrusion tasks\n",
        "def create_word_intrusion_tasks(topic_words, num_tasks=5):\n",
        "    tasks = []\n",
        "\n",
        "    topic_ids = list(topic_words.keys())\n",
        "\n",
        "    for _ in range(num_tasks):\n",
        "        # Select a topic randomly\n",
        "        true_topic = random.choice(topic_ids)\n",
        "        words = [word for word, _ in topic_words[true_topic][:top_n]]\n",
        "\n",
        "        # Select an intruder word from another topic\n",
        "        other_topics = [t for t in topic_ids if t != true_topic and topic_words[t]]\n",
        "        intruder_topic = random.choice(other_topics)\n",
        "        intruder_word = random.choice(topic_words[intruder_topic])[0]\n",
        "\n",
        "        # Add intruder and shuffle\n",
        "        options = words + [intruder_word]\n",
        "        random.shuffle(options)\n",
        "\n",
        "        task = {\n",
        "            \"topic_id\": true_topic,\n",
        "            \"options\": options,\n",
        "            \"intruder\": intruder_word\n",
        "        }\n",
        "\n",
        "        tasks.append(task)\n",
        "\n",
        "    return tasks\n",
        "\n",
        "# Step 5: Create and display tasks\n",
        "tasks = create_word_intrusion_tasks(topic_words, num_tasks=5)\n",
        "\n",
        "# Step 6: Print tasks\n",
        "for i, task in enumerate(tasks, 1):\n",
        "    print(f\"Task {i} (Topic ID: {task['topic_id']}):\")\n",
        "    print(\"Which word does NOT belong?\")\n",
        "    for word in task['options']:\n",
        "        print(f\" - {word}\")\n",
        "    print(f\"[Correct answer: {task['intruder']}]\")  # Hide this in real evaluations!\n",
        "    print(\"-\" * 40)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
